{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import pickle,os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from nets import Nets\n",
    "from utils import *\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "from postprocessing import *\n",
    "from analysis import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_root = \"/Users/daniellengyel/flat_sharp/flat_sharp/experiments/{}\" #\"/Users/daniellengyel/flat_sharp/gaussian/gaussian_experiments/Apr03_17-38-00_Daniels-MacBook-Pro-4.local\"\n",
    "# configs = get_configs(experiment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1587329968.469613\n",
      "1587329968.46856\n",
      "1587329968.469613\n",
      "1587329968.46856\n"
     ]
    }
   ],
   "source": [
    "experiment_folder = exp_root.format(\"MNIST/Apr19_22-59-17_Daniels-MacBook-Pro-4.local\")\n",
    "\n",
    "two_sampling = get_stuff(experiment_folder)\n",
    "two_models = get_all_models(experiment_folder, -1)\n",
    "two_resampling_idx = get_sample_idxs(experiment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1587346647.882677\n"
     ]
    }
   ],
   "source": [
    "experiment_folder = exp_root.format(\"MNIST/Apr20_03-37-18_Daniels-MacBook-Pro-4.local\")\n",
    "no_sampling = get_stuff(experiment_folder)\n",
    "no_models = get_all_models(experiment_folder, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1587513216.9390788\n",
      "1587513217.363613\n",
      "1587513215.967423\n",
      "1587513216.469122\n"
     ]
    }
   ],
   "source": [
    "\n",
    "experiment_folder = exp_root.format(\"MNIST/Apr22_01-53-27_Daniels-MacBook-Pro-4.local\")\n",
    "kish_lower_stuff = get_stuff(experiment_folder)\n",
    "kish_lower_models = get_all_models(experiment_folder, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1587513260.341572\n",
      "1587513258.1924942\n",
      "1587513267.032066\n",
      "1587513264.4261959\n",
      "1587513261.812853\n",
      "1587513263.2589061\n",
      "1587513265.730539\n",
      "1587513258.653904\n",
      "1587513268.094351\n",
      "1587513269.228578\n"
     ]
    }
   ],
   "source": [
    "\n",
    "experiment_folder = exp_root.format(\"MNIST/Apr22_01-53-58_Daniels-MacBook-Pro-4.local\")\n",
    "more_stuff = get_stuff(experiment_folder)\n",
    "more_models = get_all_models(experiment_folder, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1587759639.9299772\n",
      "1587759639.714095\n",
      "1587759639.9299772\n",
      "1587759639.714095\n"
     ]
    }
   ],
   "source": [
    "\n",
    "experiment_folder = exp_root.format(\"CIFAR10/Apr24_22-20-23_Daniels-MacBook-Pro-4.local\")\n",
    "CIFAR10_stuff = get_stuff(experiment_folder)\n",
    "CIFAR10_models = get_all_models(experiment_folder, -1)\n",
    "CIFAR10_resampling_idx = get_sample_idxs(experiment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: tsne could not be found\n",
      "Error: runs could not be found\n",
      "Error: trace could not be found\n",
      "1587922951.9470038\n",
      "1587922951.816041\n",
      "1587922951.815885\n",
      "1587922951.9470038\n",
      "1587922951.816041\n",
      "1587922951.815885\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "experiment_folder = exp_root.format(\"FashionMNIST/Apr26_19-42-22_Daniels-MacBook-Pro-4.local\")\n",
    "fashion_stuff = get_stuff(experiment_folder)\n",
    "fashion_models = get_all_models(experiment_folder, -1)\n",
    "fashion_resampling_idx = get_sample_idxs(experiment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1587969731.521131\n",
      "1587959691.3295202\n",
      "1587959771.2839198\n",
      "1587938682.237956\n",
      "1587979969.839093\n",
      "1587949916.310493\n",
      "1587979978.0448792\n",
      "1587979863.996534\n",
      "1587979752.008478\n",
      "1587949901.417682\n",
      "1587979875.9594638\n",
      "1587979762.577444\n",
      "1587959680.130168\n",
      "1587938676.958861\n",
      "1587949851.291446\n",
      "1587949952.543494\n",
      "1587938668.618131\n",
      "1587959718.445655\n",
      "1587979679.813218\n",
      "1587969642.196658\n",
      "1587969665.7071118\n",
      "1587949851.510209\n",
      "1587979950.331296\n",
      "1587949904.895765\n",
      "1587938675.3940868\n",
      "1587959776.781079\n",
      "1587980060.502008\n",
      "1587959724.059208\n",
      "1587959802.4893582\n",
      "1587938678.520759\n",
      "1587969614.3948228\n",
      "1587950011.623552\n",
      "1587949897.7914429\n",
      "1587949874.0319622\n",
      "1587979966.486805\n",
      "1587959691.356412\n",
      "1587938674.130773\n",
      "1587938684.189412\n",
      "1587938670.7361999\n",
      "1587949881.584672\n",
      "1587969718.0564501\n",
      "1587959717.2117991\n",
      "1587969637.918807\n",
      "1587969725.9068859\n",
      "1587969555.326112\n",
      "1587979959.526977\n",
      "1587938669.357399\n",
      "1587938680.006163\n",
      "1587969760.3500469\n",
      "1587949838.352247\n",
      "1587969725.874828\n",
      "1587959769.9323468\n",
      "1587938685.4067688\n",
      "1587938672.734766\n",
      "1587959802.4950268\n",
      "1587969735.687742\n",
      "1587969726.714976\n",
      "1587959778.332762\n",
      "1587949956.446914\n",
      "1587979708.3244438\n",
      "1587969731.521131\n",
      "1587959691.3295202\n",
      "1587959771.2839198\n",
      "1587938682.237956\n",
      "1587979969.839093\n",
      "1587949916.310493\n",
      "1587979978.0448792\n",
      "1587979863.996534\n",
      "1587979752.008478\n",
      "1587949901.417682\n",
      "1587979875.9594638\n",
      "1587979762.577444\n",
      "1587959680.130168\n",
      "1587938676.958861\n",
      "1587949851.291446\n",
      "1587949952.543494\n",
      "1587938668.618131\n",
      "1587959718.445655\n",
      "1587979679.813218\n",
      "1587969642.196658\n",
      "1587969665.7071118\n",
      "1587949851.510209\n",
      "1587979950.331296\n",
      "1587949904.895765\n",
      "1587938675.3940868\n",
      "1587959776.781079\n",
      "1587980060.502008\n",
      "1587959724.059208\n",
      "1587959802.4893582\n",
      "1587938678.520759\n",
      "1587969614.3948228\n",
      "1587950011.623552\n",
      "1587949897.7914429\n",
      "1587949874.0319622\n",
      "1587979966.486805\n",
      "1587959691.356412\n",
      "1587938674.130773\n",
      "1587938684.189412\n",
      "1587938670.7361999\n",
      "1587949881.584672\n",
      "1587969718.0564501\n",
      "1587959717.2117991\n",
      "1587969637.918807\n",
      "1587969725.9068859\n",
      "1587969555.326112\n",
      "1587979959.526977\n",
      "1587938669.357399\n",
      "1587938680.006163\n",
      "1587969760.3500469\n",
      "1587949838.352247\n",
      "1587969725.874828\n",
      "1587959769.9323468\n",
      "1587938685.4067688\n",
      "1587938672.734766\n",
      "1587959802.4950268\n",
      "1587969735.687742\n",
      "1587969726.714976\n",
      "1587959778.332762\n",
      "1587949956.446914\n",
      "1587979708.3244438\n"
     ]
    }
   ],
   "source": [
    "# get all sorts of data\n",
    "exp_dict = {}\n",
    "\n",
    "experiment_folder = exp_root.format(\"FashionMNIST/Apr27_00-03-27_Daniels-MacBook-Pro-4.local\")\n",
    "exp_dict[\"stuff\"] = get_stuff(experiment_folder)\n",
    "exp_dict[\"models\"] = get_all_models(experiment_folder, -1)\n",
    "exp_dict[\"resampling_idxs\"] = get_sample_idxs(experiment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Mean Std Trace</th>\n",
       "      <th>Mean Test Acc</th>\n",
       "      <th>Mean Trace</th>\n",
       "      <th>Mean Train Loss</th>\n",
       "      <th>Test Acc/Trace Correlation</th>\n",
       "      <th>Train Loss/Trace Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.421632</th>\n",
       "      <th>128</th>\n",
       "      <th>0</th>\n",
       "      <td>5.108050</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>27.649789</td>\n",
       "      <td>0.194319</td>\n",
       "      <td>-0.998722</td>\n",
       "      <td>-0.367454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.369053</th>\n",
       "      <th>128</th>\n",
       "      <th>0</th>\n",
       "      <td>3.985783</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>27.610197</td>\n",
       "      <td>0.209487</td>\n",
       "      <td>-0.955610</td>\n",
       "      <td>-0.278504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.684526</th>\n",
       "      <th>128</th>\n",
       "      <th>0</th>\n",
       "      <td>5.463909</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>23.863805</td>\n",
       "      <td>0.172499</td>\n",
       "      <td>-0.524813</td>\n",
       "      <td>-0.805467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.631947</th>\n",
       "      <th>128</th>\n",
       "      <th>0</th>\n",
       "      <td>5.025798</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>26.132166</td>\n",
       "      <td>0.179371</td>\n",
       "      <td>-0.938434</td>\n",
       "      <td>-0.467051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.474211</th>\n",
       "      <th>128</th>\n",
       "      <th>0</th>\n",
       "      <td>4.751749</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>27.794165</td>\n",
       "      <td>0.198997</td>\n",
       "      <td>-0.886385</td>\n",
       "      <td>-0.355652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.789684</th>\n",
       "      <th>128</th>\n",
       "      <th>0</th>\n",
       "      <td>4.112320</td>\n",
       "      <td>0.876667</td>\n",
       "      <td>22.564009</td>\n",
       "      <td>0.157192</td>\n",
       "      <td>-0.234367</td>\n",
       "      <td>0.550897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.579368</th>\n",
       "      <th>128</th>\n",
       "      <th>0</th>\n",
       "      <td>4.345401</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>24.532815</td>\n",
       "      <td>0.196615</td>\n",
       "      <td>-0.768940</td>\n",
       "      <td>-0.480792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.106158</th>\n",
       "      <th>128</th>\n",
       "      <th>0</th>\n",
       "      <td>16.269896</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>41.549845</td>\n",
       "      <td>0.313692</td>\n",
       "      <td>-0.427640</td>\n",
       "      <td>0.808183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.737105</th>\n",
       "      <th>128</th>\n",
       "      <th>0</th>\n",
       "      <td>4.019600</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>21.499106</td>\n",
       "      <td>0.178355</td>\n",
       "      <td>0.782440</td>\n",
       "      <td>0.951076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.526789</th>\n",
       "      <th>128</th>\n",
       "      <th>0</th>\n",
       "      <td>5.990123</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>27.445437</td>\n",
       "      <td>0.198126</td>\n",
       "      <td>-0.518591</td>\n",
       "      <td>0.158448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.158737</th>\n",
       "      <th>128</th>\n",
       "      <th>0</th>\n",
       "      <td>13.277259</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>34.135729</td>\n",
       "      <td>0.289841</td>\n",
       "      <td>-0.329688</td>\n",
       "      <td>-0.829714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.842263</th>\n",
       "      <th>128</th>\n",
       "      <th>0</th>\n",
       "      <td>4.079373</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>20.626546</td>\n",
       "      <td>0.160973</td>\n",
       "      <td>-0.673869</td>\n",
       "      <td>0.134897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.263895</th>\n",
       "      <th>128</th>\n",
       "      <th>0</th>\n",
       "      <td>12.077250</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>32.902826</td>\n",
       "      <td>0.241014</td>\n",
       "      <td>-0.924931</td>\n",
       "      <td>0.277984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.894842</th>\n",
       "      <th>128</th>\n",
       "      <th>0</th>\n",
       "      <td>3.390873</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>20.374952</td>\n",
       "      <td>0.152753</td>\n",
       "      <td>-0.950336</td>\n",
       "      <td>0.029520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.947421</th>\n",
       "      <th>128</th>\n",
       "      <th>0</th>\n",
       "      <td>3.622754</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>20.293483</td>\n",
       "      <td>0.157536</td>\n",
       "      <td>0.428381</td>\n",
       "      <td>0.905199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.211316</th>\n",
       "      <th>128</th>\n",
       "      <th>0</th>\n",
       "      <td>8.827257</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>32.170097</td>\n",
       "      <td>0.253188</td>\n",
       "      <td>0.277179</td>\n",
       "      <td>-0.983054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.316474</th>\n",
       "      <th>128</th>\n",
       "      <th>0</th>\n",
       "      <td>6.238928</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>29.529981</td>\n",
       "      <td>0.224252</td>\n",
       "      <td>-0.915296</td>\n",
       "      <td>-0.329986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000000</th>\n",
       "      <th>128</th>\n",
       "      <th>0</th>\n",
       "      <td>2.767099</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>18.750233</td>\n",
       "      <td>0.121428</td>\n",
       "      <td>-0.273813</td>\n",
       "      <td>0.525320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.053579</th>\n",
       "      <th>128</th>\n",
       "      <th>0</th>\n",
       "      <td>21.773641</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>46.811487</td>\n",
       "      <td>0.362237</td>\n",
       "      <td>0.030459</td>\n",
       "      <td>0.470866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.001000</th>\n",
       "      <th>128</th>\n",
       "      <th>0</th>\n",
       "      <td>4.306785</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>7.477638</td>\n",
       "      <td>1.793613</td>\n",
       "      <td>-0.967979</td>\n",
       "      <td>-0.861431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Mean Std Trace  Mean Test Acc  Mean Trace  Mean Train Loss  \\\n",
       "0.421632 128 0        5.108050       0.890000   27.649789         0.194319   \n",
       "0.369053 128 0        3.985783       0.876667   27.610197         0.209487   \n",
       "0.684526 128 0        5.463909       0.876667   23.863805         0.172499   \n",
       "0.631947 128 0        5.025798       0.876667   26.132166         0.179371   \n",
       "0.474211 128 0        4.751749       0.876667   27.794165         0.198997   \n",
       "0.789684 128 0        4.112320       0.876667   22.564009         0.157192   \n",
       "0.579368 128 0        4.345401       0.873333   24.532815         0.196615   \n",
       "0.106158 128 0       16.269896       0.873333   41.549845         0.313692   \n",
       "0.737105 128 0        4.019600       0.873333   21.499106         0.178355   \n",
       "0.526789 128 0        5.990123       0.873333   27.445437         0.198126   \n",
       "0.158737 128 0       13.277259       0.873333   34.135729         0.289841   \n",
       "0.842263 128 0        4.079373       0.873333   20.626546         0.160973   \n",
       "0.263895 128 0       12.077250       0.873333   32.902826         0.241014   \n",
       "0.894842 128 0        3.390873       0.873333   20.374952         0.152753   \n",
       "0.947421 128 0        3.622754       0.870000   20.293483         0.157536   \n",
       "0.211316 128 0        8.827257       0.870000   32.170097         0.253188   \n",
       "0.316474 128 0        6.238928       0.870000   29.529981         0.224252   \n",
       "1.000000 128 0        2.767099       0.870000   18.750233         0.121428   \n",
       "0.053579 128 0       21.773641       0.866667   46.811487         0.362237   \n",
       "0.001000 128 0        4.306785       0.470000    7.477638         1.793613   \n",
       "\n",
       "                Test Acc/Trace Correlation  Train Loss/Trace Correlation  \n",
       "0.421632 128 0                   -0.998722                     -0.367454  \n",
       "0.369053 128 0                   -0.955610                     -0.278504  \n",
       "0.684526 128 0                   -0.524813                     -0.805467  \n",
       "0.631947 128 0                   -0.938434                     -0.467051  \n",
       "0.474211 128 0                   -0.886385                     -0.355652  \n",
       "0.789684 128 0                   -0.234367                      0.550897  \n",
       "0.579368 128 0                   -0.768940                     -0.480792  \n",
       "0.106158 128 0                   -0.427640                      0.808183  \n",
       "0.737105 128 0                    0.782440                      0.951076  \n",
       "0.526789 128 0                   -0.518591                      0.158448  \n",
       "0.158737 128 0                   -0.329688                     -0.829714  \n",
       "0.842263 128 0                   -0.673869                      0.134897  \n",
       "0.263895 128 0                   -0.924931                      0.277984  \n",
       "0.894842 128 0                   -0.950336                      0.029520  \n",
       "0.947421 128 0                    0.428381                      0.905199  \n",
       "0.211316 128 0                    0.277179                     -0.983054  \n",
       "0.316474 128 0                   -0.915296                     -0.329986  \n",
       "1.000000 128 0                   -0.273813                      0.525320  \n",
       "0.053579 128 0                    0.030459                      0.470866  \n",
       "0.001000 128 0                   -0.967979                     -0.861431  "
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_df = get_end_stats(exp_dict[\"stuff\"], [\"learning_rate\", \"batch_train_size\", \"softmax_beta\"], {\"softmax_beta\":0})\n",
    "stats_df.sort_values(by=\"Mean Test Acc\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def _get_plot_values(exp_dict, X_axis, Y_axis, filter_dict={}):\n",
    "    stat_names = [\"Mean Std Trace\", \"Mean Test Acc\", \"Mean Trace\", \"Mean Train Loss\", \"Test Acc/Trace Correlation\", \"Train Loss/Trace Correlation\"]\n",
    "    key_label_names = []\n",
    "    \n",
    "    if X_axis not in stat_names:\n",
    "        key_label_names.append(X_axis)\n",
    "    if Y_axis not in stat_names:\n",
    "        key_label_names.append(Y_axis)\n",
    "\n",
    "    stats_df = get_end_stats(exp_dict[\"stuff\"], key_label_names, filter_dict)\n",
    "    \n",
    "    X_values = []\n",
    "    Y_values = []\n",
    "    \n",
    "    for idx, row in stats_df.iterrows():\n",
    "        if X_axis not in stat_names:\n",
    "            X_values.append(float(idx[0]))\n",
    "        else:\n",
    "            X_values.append(float(row[X_axis]))\n",
    "        if Y_axis not in stat_names:\n",
    "            Y_values.append(float(idx[-1]))\n",
    "        else:\n",
    "            Y_values.append(float(row[Y_axis]))\n",
    "    return X_values, Y_values\n",
    "\n",
    "def plot_end_stats(exp_dict, X_axis, Y_axis, filter_dict={}, X_axis_bounds=None, Y_axis_bounds=None):\n",
    "    \n",
    "    plots = []\n",
    "    plots_names = []\n",
    "    \n",
    "    # get template filter_dict which does not neeed to be iterated\n",
    "    # get keys which do need to be iterated \n",
    "    template_filter_dict = {}\n",
    "    grid_search_keys = []\n",
    "    grid_search_values = []\n",
    "    for k in filter_dict:\n",
    "        k_split = k.split(\":\")\n",
    "        if k_split[-1] == \"special\":\n",
    "            grid_search_keys.append(k_split[0])\n",
    "            grid_search_values.append(filter_dict[k])\n",
    "        else:\n",
    "            template_filter_dict[k] = filter_dict[k]\n",
    "            \n",
    "    # do grid search over the ones which do need to be iterated\n",
    "    if len(grid_search_keys) != 0:\n",
    "        for c in itertools.product(*grid_search_values):\n",
    "            curr_plt_name = []\n",
    "            for i in range(len(grid_search_keys)):\n",
    "                template_filter_dict[grid_search_keys[i]] = c[i]\n",
    "                curr_plt_name.append(\"{}:{} \".format(grid_search_keys[i], c[i]))\n",
    "\n",
    "            X_values, Y_values = _get_plot_values(exp_dict, X_axis, Y_axis, filter_dict=template_filter_dict)\n",
    "            plots.append(plt.scatter(X_values, Y_values))\n",
    "            plots_names.append(\"\".join(curr_plt_name)[:-1])\n",
    "    else:\n",
    "        X_values, Y_values = _get_plot_values(exp_dict, X_axis, Y_axis, filter_dict=template_filter_dict)\n",
    "        plots.append(plt.scatter(X_values, Y_values))\n",
    "        plots_names.append(\"\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    plt.legend(tuple(plots),\n",
    "       plots_names,\n",
    "       scatterpoints=1,\n",
    "       loc='lower left',\n",
    "       ncol=3,\n",
    "       fontsize=8)\n",
    "    \n",
    "    plt.xlabel(X_axis)\n",
    "    plt.ylabel(Y_axis)\n",
    "    if X_axis_bounds is not None:\n",
    "        plt.xlim(X_axis_bounds) \n",
    "    if Y_axis_bounds is not None:\n",
    "        plt.ylim(Y_axis_bounds)    \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_end_stats(exp_dict, X_axis=\"Mean Trace\", Y_axis=\"Mean Train Loss\", filter_dict={\"softmax_beta:special\": [-100, 0, 100]}, Y_axis_bounds=(0, 0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stuff = CIFAR10_stuff\n",
    "\n",
    "def get_runs_plots(exp_dict, var_name=\"Kish\", running_average_gamma=0.6):\n",
    "    exp_runs = exp_dict[\"stuff\"][\"runs\"]\n",
    "    for i in exp_runs:\n",
    "        \n",
    "        plot_list = [0]\n",
    "\n",
    "        print(i)\n",
    "\n",
    "        for step in sorted(exp_runs[i], key=lambda x: int(x)):\n",
    "            try:\n",
    "                # going down the tree with node names given by var_name.split(\"/\")\n",
    "                curr_dict = exp_runs[i][step]\n",
    "                var_name_split = var_name.split(\"/\")\n",
    "                for n in var_name_split:\n",
    "                    curr_dict = curr_dict[n]\n",
    "                    \n",
    "                if \"net\" in curr_dict:\n",
    "                    num_nets = int(max(curr_dict[\"net\"], key=lambda x: int(x))) + 1 # +1 bc zero indexed\n",
    "                    to_append = np.array([curr_dict[\"net\"][str(nn)] for nn in range(num_nets)])\n",
    "                    \n",
    "                else:\n",
    "                    to_append = curr_dict[\"\"]\n",
    "                to_append = plot_list[-1]*(1 - running_average_gamma) + running_average_gamma*to_append\n",
    "                plot_list.append(to_append)\n",
    "            except:\n",
    "                print(\"No {} for step {}\".format(var_name, step))\n",
    "        plt.plot(plot_list[1:])\n",
    "        plt.show()\n",
    "            \n",
    "    return tmp_list\n",
    "\n",
    "# kish = get_runs_plot(stuff[\"runs\"], [\"Kish\"])\n",
    "# weight_var_trace = get_runs_plot(stuff[\"runs\"], [\"WeightVarTrace\"])\n",
    "\n",
    "weight_var_trace = get_runs_plots(exp_dict, \"Potential/curr\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(kish)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(weight_var_trace)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_analysis(exp_dict, X_axis, Y_axis):\n",
    "\n",
    "    marker_list = [\"o\", \"v\", \"^\", \"<\", \">\", \"1\", \"2\", \"3\", \"4\", \"8\", \"s\", \"p\", \"P\", \"*\", \"h\", \"H\", \"+\", \"x\", \"X\", \"D\", \"d\", \"|\", \"_\"]\n",
    "\n",
    "\n",
    "\n",
    "    stuff = exp_dict[\"stuff\"]\n",
    "    run_data = stuff[\"runs\"]\n",
    "    trace = stuff[\"trace\"]\n",
    "    \n",
    "    \n",
    "    for exp_id in stuff[\"configs\"]:\n",
    "        plots = []\n",
    "        plots_names = []\n",
    "                \n",
    "        num_nets = stuff[\"configs\"][exp_id][\"num_nets\"]\n",
    "        num_steps = max(run_data[exp_id], key=lambda x: int(x))\n",
    "\n",
    "        beta = stuff[\"configs\"][exp_id][\"softmax_beta\"]\n",
    "\n",
    "        print(beta)\n",
    "\n",
    "        models = exp_dict[\"models\"][exp_id] #CIFAR10_models\n",
    "        models_vecs = np.array([get_params_vec(models[str(nn)]).detach().numpy() for nn in range(num_nets)])\n",
    "        \n",
    "        clustering = DBSCAN(eps=5, min_samples=2).fit(models_vecs)\n",
    "        l = clustering.labels_\n",
    "\n",
    "\n",
    "\n",
    "#         Y = [run_data[i][4999][\"Potential\"][\"total\"][\"net\"][str(nn)] for nn in range(50)]\n",
    "    #     Y = [eigs[i][str(nn)][0][0] for nn in range(50)]\n",
    "        Y = np.array([np.mean(trace[exp_id][str(nn)]) for nn in range(num_nets)])\n",
    "\n",
    "#         X = np.array([run_data[id_exp][4999][\"Loss\"][\"train\"][\"net\"][str(nn)] for nn in range(50)])\n",
    "        X = np.array([run_data[exp_id][num_steps][\"Accuracy\"][\"net\"][str(nn)] for nn in range(num_nets)])\n",
    "\n",
    "#         last_sampling_step = sorted(two_resampling_idx[id_exp], key=lambda x: int(x))[-2]\n",
    "#         last_resamples = two_resampling_idx[id_exp][last_sampling_step]\n",
    "#         res_idx_to_marker = {res_idx: marker_list[i] for i, res_idx in enumerate(set(last_resamples))}\n",
    "\n",
    "        label_set = set(l)\n",
    "#         for l in range(50):\n",
    "#             plots_names.append(l[nn])\n",
    "#             plots.append(plt.scatter(X[nn], Y[nn], marker=\"o\"))#res_idx_to_marker[last_resamples[nn]])\n",
    "        for label in label_set:\n",
    "            plots_names.append(label)\n",
    "            plots.append(plt.scatter(X[l == label], Y[l == label], marker=\"o\"))#res_idx_to_marker[last_resamples[nn]])\n",
    "\n",
    "\n",
    "        plt.legend(tuple(plots),\n",
    "               plots_names,\n",
    "               scatterpoints=1,\n",
    "               loc='lower left',\n",
    "               ncol=3,\n",
    "               fontsize=8)\n",
    "\n",
    "\n",
    "\n",
    "        plt.xlabel(\"loss\")\n",
    "        plt.ylabel(\"trace\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-50\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_analysis(exp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some analysis of final distance... \n",
    "\n",
    "show low loss path between. distance. what else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis \n",
    "\n",
    "Plots (FUNCTION):\n",
    "ACC vs Trace\n",
    "Train Loss vs Trace\n",
    "ACC vs Train loss\n",
    "Mean/COV ACC -- MEAN/COV TRACE -- MEAN/COV TRAIN LOSS (CORRELATIONS FOR ALL OF THESE)\n",
    "\n",
    "\n",
    "Plot weightspace and input space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-3f8da419e8db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mplots_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mrun_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "plots_names = []\n",
    "plots = []\n",
    "\n",
    "for r_data in [CIFAR10_stuff]:\n",
    "    for i in r_data:\n",
    "        \n",
    "        try:\n",
    "            beta = configs[i][\"softmax_beta\"]\n",
    "        except:\n",
    "            beta = 0\n",
    "        plots_names.append(str(beta))\n",
    "               \n",
    "        run_data = r_data[0]\n",
    "        trace = r_data[1]\n",
    "\n",
    "        Y = [run_data[i][4999][\"Potential\"][\"total\"][\"net\"][str(nn)] for nn in range(50)]\n",
    "    #     Y = [eigs[i][str(nn)][0][0] for nn in range(50)]\n",
    "        Y = [np.mean(trace[i][str(nn)]) for nn in range(50)]\n",
    "\n",
    "        X = [run_data[i][4999][\"Loss\"][\"train\"][\"net\"][str(nn)] for nn in range(50)]\n",
    "#         X = [run_data[i][5000][\"Accuracy\"][\"net\"][str(nn)] for nn in range(50)]\n",
    "\n",
    "        plots.append(plt.scatter(np.array(X), np.array(Y)))\n",
    "\n",
    "        \n",
    "plt.legend(tuple(plots),\n",
    "       plots_names,\n",
    "       scatterpoints=1,\n",
    "       loc='lower left',\n",
    "       ncol=3,\n",
    "       fontsize=8)\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"loss\")\n",
    "plt.ylabel(\"trace\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7., 3., 6., 5., 4., 6., 2., 1., 1., 1.]),\n",
       " array([ 89.99026489,  98.92023621, 107.85020752, 116.78017883,\n",
       "        125.71015015, 134.64012146, 143.57009277, 152.50006409,\n",
       "        161.4300354 , 170.36000671, 179.28997803]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = [127.37311553955078, 97.93885803222656, 158.1538848876953, 132.8297119140625, 116.41229248046875, 112.09640502929688, 131.17662048339844, 140.27606201171875, 104.38414764404297, 155.23452758789062, 136.9343719482422, 130.11436462402344, 121.89076232910156, 97.82975006103516, 121.72054290771484, 104.34451293945312, 169.51600646972656, 91.82275390625, 134.81613159179688, 136.09010314941406, 98.46024322509766, 121.43125915527344, 88.79573059082031, 118.9525146484375, 106.84197998046875, 93.86476135253906, 166.26219177246094, 111.35343170166016, 133.33021545410156, 107.85846710205078, 111.2021255493164, 125.52880859375]\n",
    "a = [148.1651153564453, 89.99545288085938, 143.13145446777344, 129.59164428710938, 133.98138427734375, 179.28997802734375, 139.3506317138672, 90.68531036376953, 107.76986694335938, 121.02605438232422, 135.69696044921875, 101.74652099609375, 115.87667083740234, 122.7598648071289, 92.99028015136719, 165.59707641601562, 118.77754211425781, 90.56055450439453, 116.37315368652344, 113.5857162475586, 89.99026489257812, 136.0908203125, 90.26911163330078, 147.79518127441406, 137.81057739257812, 156.77203369140625, 118.10494232177734, 98.78507995605469, 133.88307189941406, 110.46347045898438, 113.26660919189453, 140.4408721923828, 99.68799591064453, 126.61418151855469, 113.37327575683594, 119.75435638427734]\n",
    "plt.hist(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n",
      "[542.9575805664062, 600.6249389648438, 534.30419921875, 553.8997802734375, 542.3204345703125, 695.7343139648438, 568.3363037109375, 559.5490112304688, 621.9464721679688, 564.6443481445312, 519.8447265625, 521.8214721679688, 515.1004638671875, 609.952392578125, 537.172607421875, 616.8250732421875, 509.0643310546875, 635.6880493164062, 471.21484375, 577.85107421875, 549.3336791992188, 599.1685180664062, 587.0655517578125, 695.5125732421875, 562.3419189453125]\n",
      "\n",
      "(-50,)\n",
      "[468.0771179199219, 457.67218017578125, 510.1174011230469, 474.34967041015625, 574.97021484375, 486.2344665527344, 606.5770874023438, 497.95318603515625, 487.2482604980469, 620.97802734375, 531.549560546875, 475.3292236328125, 438.49078369140625, 439.6686706542969, 546.5634765625, 573.0172119140625, 548.7708740234375, 514.002685546875]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean Std Trace</th>\n",
       "      <th>Mean Test Acc</th>\n",
       "      <th>Mean Trace</th>\n",
       "      <th>Mean Train Loss</th>\n",
       "      <th>Test Acc/Trace Correlation</th>\n",
       "      <th>Train Loss/Trace Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>54.889864</td>\n",
       "      <td>0.5996</td>\n",
       "      <td>581.138967</td>\n",
       "      <td>0.869805</td>\n",
       "      <td>0.173417</td>\n",
       "      <td>-0.026742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-50</th>\n",
       "      <td>56.388127</td>\n",
       "      <td>0.6072</td>\n",
       "      <td>527.883488</td>\n",
       "      <td>0.925043</td>\n",
       "      <td>0.138459</td>\n",
       "      <td>-0.366431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Mean Std Trace  Mean Test Acc  Mean Trace  Mean Train Loss  \\\n",
       " 50       54.889864         0.5996  581.138967         0.869805   \n",
       "-50       56.388127         0.6072  527.883488         0.925043   \n",
       "\n",
       "     Test Acc/Trace Correlation  Train Loss/Trace Correlation  \n",
       " 50                    0.173417                     -0.026742  \n",
       "-50                    0.138459                     -0.366431  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next step, get accuracy here... sucks we can't get it otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-64b5902c502c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfigs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconfig_to_id_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config_to_id_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_to_id_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "configs = get_configs(experiment_folder)\n",
    "config_to_id_map = get_config_to_id_map(configs)\n",
    "ids = get_ids(config_to_id_map, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'batch_test_size': 100,\n",
    " 'batch_train_size': 16,\n",
    " 'ess_threshold': None,\n",
    " 'learning_rate': 0.001,\n",
    " 'momentum': 0,\n",
    " 'net_name': 'SimpleNet',\n",
    " 'net_params': None, #[2, 2, 256, 1],\n",
    " 'num_steps': 100,\n",
    " 'num_nets': 100,\n",
    " 'softmax_beta': None, #0,\n",
    " 'torch_random_seed': 1,\n",
    " 'sampling_tau': None,\n",
    " 'weight_type': 'loss_gradient_weights'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in sorted(tsne_dict):\n",
    "    print(i)\n",
    "    plt.scatter(tsne_dict[i][:, 0], tsne_dict[i][:, 1])\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tsne_dict = get_tsne_dict(experiment_folder, curr_dir)\n",
    "for i in sorted(tsne_dict):\n",
    "    print(i)\n",
    "    plt.scatter(tsne_dict[i][:, 0], tsne_dict[i][:, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "models, sampled_idx = get_models(os.path.join(experiment_folder, \"resampling\", curr_dir), 0)\n",
    "models_vecs = np.array([get_params_vec(m) for m in models.values()])\n",
    "\n",
    "shortest_distance = float(\"inf\")\n",
    "shortest_pair = (None, None)\n",
    "\n",
    "largest_distance = -float(\"inf\")\n",
    "largest_pair = (None, None)\n",
    "for i in range(len(models_vecs)):\n",
    "    for j in range(i + 1, len(models_vecs)):\n",
    "        if np.linalg.norm(models_vecs[i] - models_vecs[j]) < shortest_distance:\n",
    "            shortest_distance = np.linalg.norm(models_vecs[i] - models_vecs[j])\n",
    "            shortest_pair = (i, j)\n",
    "        if np.linalg.norm(models_vecs[i] - models_vecs[j]) > largest_distance:\n",
    "            largest_distance = np.linalg.norm(models_vecs[i] - models_vecs[j])\n",
    "            largest_pair = (i, j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_distance\n",
    "np.linalg.norm(models_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(experiment_folder, \"data.pkl\"), \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "train_loader = DataLoader(data[0], batch_size=len(data[0]), shuffle=True)  # fix the batch size\n",
    "test_loader = DataLoader(data[1], batch_size=len(data[1]))\n",
    "\n",
    "largest_arr = []\n",
    "largest_acc = []\n",
    "shortest_arr = []\n",
    "shortest_acc = []\n",
    "\n",
    "largest_mags = []\n",
    "shortest_mags = []\n",
    "\n",
    "step_dir = get_all_model_steps(os.path.join(experiment_folder, \"resampling\", curr_dir))\n",
    "for step in sorted(step_dir):\n",
    "    print(step)\n",
    "    models, sampled_idx = get_models(os.path.join(experiment_folder, \"resampling\", curr_dir), step)\n",
    "    models_vecs = np.array([get_params_vec(m) for m in models.values()])\n",
    "    \n",
    "    largest_arr.append(np.linalg.norm(models_vecs[largest_pair[0]] - models_vecs[largest_pair[1]]))\n",
    "    shortest_arr.append(np.linalg.norm(models_vecs[shortest_pair[0]] - models_vecs[shortest_pair[1]]))\n",
    "    \n",
    "    largest_acc.append([get_net_accuracy(models[str(largest_pair[0])], test_loader), get_net_accuracy(models[str(largest_pair[1])], test_loader)])\n",
    "    shortest_acc.append([get_net_accuracy(models[str(shortest_pair[0])], test_loader), get_net_accuracy(models[str(shortest_pair[1])], test_loader)])\n",
    "\n",
    "\n",
    "    largest_mags.append([np.linalg.norm(models_vecs[largest_pair[0]]), np.linalg.norm(models_vecs[largest_pair[1]])])\n",
    "    shortest_mags.append([np.linalg.norm(models_vecs[shortest_pair[0]]),np.linalg.norm(models_vecs[shortest_pair[1]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(sorted(step_dir)), largest_arr )\n",
    "plt.show()\n",
    "plt.plot(list(sorted(step_dir)), np.array(largest_acc)[:, 0])\n",
    "plt.plot(list(sorted(step_dir)), np.array(largest_acc)[:, 1])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(list(sorted(step_dir)), np.array(shortest_acc)[:, 0])\n",
    "plt.plot(list(sorted(step_dir)), np.array(shortest_acc)[:, 1])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(list(sorted(step_dir)), np.array(largest_mags)[:, 0])\n",
    "plt.plot(list(sorted(step_dir)), np.array(largest_mags)[:, 1])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(list(sorted(step_dir)), np.array(shortest_mags)[:, 0])\n",
    "plt.plot(list(sorted(step_dir)), np.array(shortest_mags)[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_dist = {}\n",
    "for i in range(len(models)):\n",
    "    for j in range(i+1, len(models)):\n",
    "        a = get_params_vec(models[str(i)])\n",
    "        b = get_params_vec(models[str(j)])\n",
    "\n",
    "        all_dist[(i, j)] = torch.norm(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (0, 2): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 3): tensor(0.0025, grad_fn=<NormBackward0>),\n",
       " (0, 4): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (0, 5): tensor(0.0025, grad_fn=<NormBackward0>),\n",
       " (0, 6): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 7): tensor(0.0040, grad_fn=<NormBackward0>),\n",
       " (0, 8): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 9): tensor(0.0032, grad_fn=<NormBackward0>),\n",
       " (0, 10): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 11): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 12): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 13): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 14): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 15): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 16): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 17): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 18): tensor(0.0022, grad_fn=<NormBackward0>),\n",
       " (0, 19): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (0, 20): tensor(0.0025, grad_fn=<NormBackward0>),\n",
       " (0, 21): tensor(0.0046, grad_fn=<NormBackward0>),\n",
       " (0, 22): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 23): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (0, 24): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 25): tensor(0.0029, grad_fn=<NormBackward0>),\n",
       " (0, 26): tensor(0.0041, grad_fn=<NormBackward0>),\n",
       " (0, 27): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 28): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 29): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 30): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 31): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 32): tensor(0.0038, grad_fn=<NormBackward0>),\n",
       " (0, 33): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (0, 34): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (0, 35): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 36): tensor(0.0019, grad_fn=<NormBackward0>),\n",
       " (0, 37): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 38): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 39): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 40): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (0, 41): tensor(0.0034, grad_fn=<NormBackward0>),\n",
       " (0, 42): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 43): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (0, 44): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (0, 45): tensor(0.0025, grad_fn=<NormBackward0>),\n",
       " (0, 46): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (0, 47): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (0, 48): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 49): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 50): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 51): tensor(0.0021, grad_fn=<NormBackward0>),\n",
       " (0, 52): tensor(0.0029, grad_fn=<NormBackward0>),\n",
       " (0, 53): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 54): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 55): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 56): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (0, 57): tensor(0.0034, grad_fn=<NormBackward0>),\n",
       " (0, 58): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 59): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (0, 60): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 61): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 62): tensor(0.0050, grad_fn=<NormBackward0>),\n",
       " (0, 63): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 64): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 65): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (0, 66): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 67): tensor(0.0039, grad_fn=<NormBackward0>),\n",
       " (0, 68): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (0, 69): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (0, 70): tensor(0.0026, grad_fn=<NormBackward0>),\n",
       " (0, 71): tensor(0.0035, grad_fn=<NormBackward0>),\n",
       " (0, 72): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 73): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (0, 74): tensor(0.0019, grad_fn=<NormBackward0>),\n",
       " (0, 75): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 76): tensor(0.0055, grad_fn=<NormBackward0>),\n",
       " (0, 77): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 78): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (0, 79): tensor(0.0035, grad_fn=<NormBackward0>),\n",
       " (0, 80): tensor(0.0028, grad_fn=<NormBackward0>),\n",
       " (0, 81): tensor(0.0031, grad_fn=<NormBackward0>),\n",
       " (0, 82): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (0, 83): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 84): tensor(0.0024, grad_fn=<NormBackward0>),\n",
       " (0, 85): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 86): tensor(0.0037, grad_fn=<NormBackward0>),\n",
       " (0, 87): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (0, 88): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 89): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 90): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 91): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (0, 92): tensor(0.0031, grad_fn=<NormBackward0>),\n",
       " (0, 93): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 94): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (0, 95): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 96): tensor(0.0032, grad_fn=<NormBackward0>),\n",
       " (0, 97): tensor(0.0037, grad_fn=<NormBackward0>),\n",
       " (0, 98): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (0, 99): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (1, 2): tensor(0.0043, grad_fn=<NormBackward0>),\n",
       " (1, 3): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (1, 4): tensor(0.0053, grad_fn=<NormBackward0>),\n",
       " (1, 5): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (1, 6): tensor(0.0047, grad_fn=<NormBackward0>),\n",
       " (1, 7): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (1, 8): tensor(0.0029, grad_fn=<NormBackward0>),\n",
       " (1, 9): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (1, 10): tensor(0.0038, grad_fn=<NormBackward0>),\n",
       " (1, 11): tensor(0.0033, grad_fn=<NormBackward0>),\n",
       " (1, 12): tensor(0.0052, grad_fn=<NormBackward0>),\n",
       " (1, 13): tensor(0.0076, grad_fn=<NormBackward0>),\n",
       " (1, 14): tensor(0.0042, grad_fn=<NormBackward0>),\n",
       " (1, 15): tensor(0.0039, grad_fn=<NormBackward0>),\n",
       " (1, 16): tensor(0.0039, grad_fn=<NormBackward0>),\n",
       " (1, 17): tensor(0.0022, grad_fn=<NormBackward0>),\n",
       " (1, 18): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (1, 19): tensor(0.0039, grad_fn=<NormBackward0>),\n",
       " (1, 20): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (1, 21): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (1, 22): tensor(0.0061, grad_fn=<NormBackward0>),\n",
       " (1, 23): tensor(0.0046, grad_fn=<NormBackward0>),\n",
       " (1, 24): tensor(0.0051, grad_fn=<NormBackward0>),\n",
       " (1, 25): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (1, 26): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (1, 27): tensor(0.0030, grad_fn=<NormBackward0>),\n",
       " (1, 28): tensor(0.0033, grad_fn=<NormBackward0>),\n",
       " (1, 29): tensor(0.0065, grad_fn=<NormBackward0>),\n",
       " (1, 30): tensor(0.0050, grad_fn=<NormBackward0>),\n",
       " (1, 31): tensor(0.0048, grad_fn=<NormBackward0>),\n",
       " (1, 32): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (1, 33): tensor(0.0055, grad_fn=<NormBackward0>),\n",
       " (1, 34): tensor(0.0051, grad_fn=<NormBackward0>),\n",
       " (1, 35): tensor(0.0065, grad_fn=<NormBackward0>),\n",
       " (1, 36): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (1, 37): tensor(0.0074, grad_fn=<NormBackward0>),\n",
       " (1, 38): tensor(0.0051, grad_fn=<NormBackward0>),\n",
       " (1, 39): tensor(0.0011, grad_fn=<NormBackward0>),\n",
       " (1, 40): tensor(0.0060, grad_fn=<NormBackward0>),\n",
       " (1, 41): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (1, 42): tensor(0.0031, grad_fn=<NormBackward0>),\n",
       " (1, 43): tensor(0.0051, grad_fn=<NormBackward0>),\n",
       " (1, 44): tensor(0.0074, grad_fn=<NormBackward0>),\n",
       " (1, 45): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (1, 46): tensor(0.0061, grad_fn=<NormBackward0>),\n",
       " (1, 47): tensor(0.0074, grad_fn=<NormBackward0>),\n",
       " (1, 48): tensor(0.0061, grad_fn=<NormBackward0>),\n",
       " (1, 49): tensor(0.0034, grad_fn=<NormBackward0>),\n",
       " (1, 50): tensor(0.0056, grad_fn=<NormBackward0>),\n",
       " (1, 51): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (1, 52): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (1, 53): tensor(0.0036, grad_fn=<NormBackward0>),\n",
       " (1, 54): tensor(0.0030, grad_fn=<NormBackward0>),\n",
       " (1, 55): tensor(0.0040, grad_fn=<NormBackward0>),\n",
       " (1, 56): tensor(0.0047, grad_fn=<NormBackward0>),\n",
       " (1, 57): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (1, 58): tensor(0.0041, grad_fn=<NormBackward0>),\n",
       " (1, 59): tensor(0.0045, grad_fn=<NormBackward0>),\n",
       " (1, 60): tensor(0.0045, grad_fn=<NormBackward0>),\n",
       " (1, 61): tensor(0.0047, grad_fn=<NormBackward0>),\n",
       " (1, 62): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (1, 63): tensor(0.0063, grad_fn=<NormBackward0>),\n",
       " (1, 64): tensor(0.0040, grad_fn=<NormBackward0>),\n",
       " (1, 65): tensor(0.0013, grad_fn=<NormBackward0>),\n",
       " (1, 66): tensor(0.0046, grad_fn=<NormBackward0>),\n",
       " (1, 67): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (1, 68): tensor(0.0037, grad_fn=<NormBackward0>),\n",
       " (1, 69): tensor(0.0053, grad_fn=<NormBackward0>),\n",
       " (1, 70): tensor(16.6314, grad_fn=<NormBackward0>),\n",
       " (1, 71): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (1, 72): tensor(0.0039, grad_fn=<NormBackward0>),\n",
       " (1, 73): tensor(0.0018, grad_fn=<NormBackward0>),\n",
       " (1, 74): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (1, 75): tensor(0.0075, grad_fn=<NormBackward0>),\n",
       " (1, 76): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (1, 77): tensor(0.0042, grad_fn=<NormBackward0>),\n",
       " (1, 78): tensor(0.0082, grad_fn=<NormBackward0>),\n",
       " (1, 79): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (1, 80): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (1, 81): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (1, 82): tensor(0.0085, grad_fn=<NormBackward0>),\n",
       " (1, 83): tensor(0.0047, grad_fn=<NormBackward0>),\n",
       " (1, 84): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (1, 85): tensor(0.0017, grad_fn=<NormBackward0>),\n",
       " (1, 86): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (1, 87): tensor(0.0040, grad_fn=<NormBackward0>),\n",
       " (1, 88): tensor(0.0040, grad_fn=<NormBackward0>),\n",
       " (1, 89): tensor(0.0050, grad_fn=<NormBackward0>),\n",
       " (1, 90): tensor(0.0034, grad_fn=<NormBackward0>),\n",
       " (1, 91): tensor(0.0043, grad_fn=<NormBackward0>),\n",
       " (1, 92): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (1, 93): tensor(0.0018, grad_fn=<NormBackward0>),\n",
       " (1, 94): tensor(0.0038, grad_fn=<NormBackward0>),\n",
       " (1, 95): tensor(0.0045, grad_fn=<NormBackward0>),\n",
       " (1, 96): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (1, 97): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (1, 98): tensor(0.0088, grad_fn=<NormBackward0>),\n",
       " (1, 99): tensor(0.0042, grad_fn=<NormBackward0>),\n",
       " (2, 3): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (2, 4): tensor(0.0050, grad_fn=<NormBackward0>),\n",
       " (2, 5): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (2, 6): tensor(0.0039, grad_fn=<NormBackward0>),\n",
       " (2, 7): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (2, 8): tensor(0.0025, grad_fn=<NormBackward0>),\n",
       " (2, 9): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (2, 10): tensor(0.0068, grad_fn=<NormBackward0>),\n",
       " (2, 11): tensor(0.0035, grad_fn=<NormBackward0>),\n",
       " (2, 12): tensor(0.0054, grad_fn=<NormBackward0>),\n",
       " (2, 13): tensor(0.0076, grad_fn=<NormBackward0>),\n",
       " (2, 14): tensor(0.0030, grad_fn=<NormBackward0>),\n",
       " (2, 15): tensor(0.0036, grad_fn=<NormBackward0>),\n",
       " (2, 16): tensor(0.0054, grad_fn=<NormBackward0>),\n",
       " (2, 17): tensor(0.0048, grad_fn=<NormBackward0>),\n",
       " (2, 18): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (2, 19): tensor(0.0021, grad_fn=<NormBackward0>),\n",
       " (2, 20): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (2, 21): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (2, 22): tensor(0.0080, grad_fn=<NormBackward0>),\n",
       " (2, 23): tensor(0.0026, grad_fn=<NormBackward0>),\n",
       " (2, 24): tensor(0.0032, grad_fn=<NormBackward0>),\n",
       " (2, 25): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (2, 26): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (2, 27): tensor(0.0024, grad_fn=<NormBackward0>),\n",
       " (2, 28): tensor(0.0028, grad_fn=<NormBackward0>),\n",
       " (2, 29): tensor(0.0060, grad_fn=<NormBackward0>),\n",
       " (2, 30): tensor(0.0060, grad_fn=<NormBackward0>),\n",
       " (2, 31): tensor(0.0024, grad_fn=<NormBackward0>),\n",
       " (2, 32): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (2, 33): tensor(0.0053, grad_fn=<NormBackward0>),\n",
       " (2, 34): tensor(0.0041, grad_fn=<NormBackward0>),\n",
       " (2, 35): tensor(0.0043, grad_fn=<NormBackward0>),\n",
       " (2, 36): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (2, 37): tensor(0.0090, grad_fn=<NormBackward0>),\n",
       " (2, 38): tensor(0.0049, grad_fn=<NormBackward0>),\n",
       " (2, 39): tensor(0.0046, grad_fn=<NormBackward0>),\n",
       " (2, 40): tensor(0.0054, grad_fn=<NormBackward0>),\n",
       " (2, 41): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (2, 42): tensor(0.0019, grad_fn=<NormBackward0>),\n",
       " (2, 43): tensor(0.0032, grad_fn=<NormBackward0>),\n",
       " (2, 44): tensor(0.0073, grad_fn=<NormBackward0>),\n",
       " (2, 45): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (2, 46): tensor(0.0054, grad_fn=<NormBackward0>),\n",
       " (2, 47): tensor(0.0070, grad_fn=<NormBackward0>),\n",
       " (2, 48): tensor(0.0083, grad_fn=<NormBackward0>),\n",
       " (2, 49): tensor(0.0032, grad_fn=<NormBackward0>),\n",
       " (2, 50): tensor(0.0055, grad_fn=<NormBackward0>),\n",
       " (2, 51): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (2, 52): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (2, 53): tensor(0.0032, grad_fn=<NormBackward0>),\n",
       " (2, 54): tensor(0.0054, grad_fn=<NormBackward0>),\n",
       " (2, 55): tensor(0.0020, grad_fn=<NormBackward0>),\n",
       " (2, 56): tensor(0.0027, grad_fn=<NormBackward0>),\n",
       " (2, 57): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (2, 58): tensor(0.0037, grad_fn=<NormBackward0>),\n",
       " (2, 59): tensor(0.0036, grad_fn=<NormBackward0>),\n",
       " (2, 60): tensor(0.0038, grad_fn=<NormBackward0>),\n",
       " (2, 61): tensor(0.0038, grad_fn=<NormBackward0>),\n",
       " (2, 62): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (2, 63): tensor(0.0065, grad_fn=<NormBackward0>),\n",
       " (2, 64): tensor(0.0059, grad_fn=<NormBackward0>),\n",
       " (2, 65): tensor(0.0050, grad_fn=<NormBackward0>),\n",
       " (2, 66): tensor(0.0028, grad_fn=<NormBackward0>),\n",
       " (2, 67): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (2, 68): tensor(0.0036, grad_fn=<NormBackward0>),\n",
       " (2, 69): tensor(0.0043, grad_fn=<NormBackward0>),\n",
       " (2, 70): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (2, 71): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (2, 72): tensor(0.0024, grad_fn=<NormBackward0>),\n",
       " (2, 73): tensor(0.0050, grad_fn=<NormBackward0>),\n",
       " (2, 74): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (2, 75): tensor(0.0067, grad_fn=<NormBackward0>),\n",
       " (2, 76): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (2, 77): tensor(0.0029, grad_fn=<NormBackward0>),\n",
       " (2, 78): tensor(0.0080, grad_fn=<NormBackward0>),\n",
       " (2, 79): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (2, 80): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (2, 81): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (2, 82): tensor(0.0076, grad_fn=<NormBackward0>),\n",
       " (2, 83): tensor(0.0029, grad_fn=<NormBackward0>),\n",
       " (2, 84): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (2, 85): tensor(0.0047, grad_fn=<NormBackward0>),\n",
       " (2, 86): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (2, 87): tensor(0.0038, grad_fn=<NormBackward0>),\n",
       " (2, 88): tensor(0.0025, grad_fn=<NormBackward0>),\n",
       " (2, 89): tensor(0.0025, grad_fn=<NormBackward0>),\n",
       " (2, 90): tensor(0.0021, grad_fn=<NormBackward0>),\n",
       " (2, 91): tensor(0.0042, grad_fn=<NormBackward0>),\n",
       " (2, 92): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (2, 93): tensor(0.0047, grad_fn=<NormBackward0>),\n",
       " (2, 94): tensor(0.0030, grad_fn=<NormBackward0>),\n",
       " (2, 95): tensor(0.0036, grad_fn=<NormBackward0>),\n",
       " (2, 96): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (2, 97): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (2, 98): tensor(0.0086, grad_fn=<NormBackward0>),\n",
       " (2, 99): tensor(0.0030, grad_fn=<NormBackward0>),\n",
       " (3, 4): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 5): tensor(0.0025, grad_fn=<NormBackward0>),\n",
       " (3, 6): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 7): tensor(0.0034, grad_fn=<NormBackward0>),\n",
       " (3, 8): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 9): tensor(0.0025, grad_fn=<NormBackward0>),\n",
       " (3, 10): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (3, 11): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (3, 12): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 13): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 14): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 15): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 16): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 17): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (3, 18): tensor(0.0018, grad_fn=<NormBackward0>),\n",
       " (3, 19): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (3, 20): tensor(0.0037, grad_fn=<NormBackward0>),\n",
       " (3, 21): tensor(0.0034, grad_fn=<NormBackward0>),\n",
       " (3, 22): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 23): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 24): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 25): tensor(0.0017, grad_fn=<NormBackward0>),\n",
       " (3, 26): tensor(0.0030, grad_fn=<NormBackward0>),\n",
       " (3, 27): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (3, 28): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (3, 29): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 30): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 31): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (3, 32): tensor(0.0026, grad_fn=<NormBackward0>),\n",
       " (3, 33): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 34): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 35): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 36): tensor(0.0028, grad_fn=<NormBackward0>),\n",
       " (3, 37): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 38): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 39): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (3, 40): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (3, 41): tensor(0.0019, grad_fn=<NormBackward0>),\n",
       " (3, 42): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (3, 43): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 44): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 45): tensor(0.0034, grad_fn=<NormBackward0>),\n",
       " (3, 46): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (3, 47): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 48): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (3, 49): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (3, 50): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 51): tensor(0.0016, grad_fn=<NormBackward0>),\n",
       " (3, 52): tensor(0.0040, grad_fn=<NormBackward0>),\n",
       " (3, 53): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 54): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (3, 55): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 56): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 57): tensor(0.0023, grad_fn=<NormBackward0>),\n",
       " (3, 58): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (3, 59): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 60): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (3, 61): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 62): tensor(0.0037, grad_fn=<NormBackward0>),\n",
       " (3, 63): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 64): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 65): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (3, 66): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (3, 67): tensor(0.0053, grad_fn=<NormBackward0>),\n",
       " (3, 68): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 69): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 70): tensor(0.0033, grad_fn=<NormBackward0>),\n",
       " (3, 71): tensor(0.0035, grad_fn=<NormBackward0>),\n",
       " (3, 72): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 73): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (3, 74): tensor(0.0024, grad_fn=<NormBackward0>),\n",
       " (3, 75): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 76): tensor(0.0072, grad_fn=<NormBackward0>),\n",
       " (3, 77): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 78): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 79): tensor(0.0026, grad_fn=<NormBackward0>),\n",
       " (3, 80): tensor(0.0040, grad_fn=<NormBackward0>),\n",
       " (3, 81): tensor(0.0021, grad_fn=<NormBackward0>),\n",
       " (3, 82): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (3, 83): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 84): tensor(0.0011, grad_fn=<NormBackward0>),\n",
       " (3, 85): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 86): tensor(0.0023, grad_fn=<NormBackward0>),\n",
       " (3, 87): tensor(16.6314, grad_fn=<NormBackward0>),\n",
       " (3, 88): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 89): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 90): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (3, 91): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 92): tensor(0.0021, grad_fn=<NormBackward0>),\n",
       " (3, 93): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 94): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 95): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (3, 96): tensor(0.0019, grad_fn=<NormBackward0>),\n",
       " (3, 97): tensor(0.0026, grad_fn=<NormBackward0>),\n",
       " (3, 98): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (3, 99): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (4, 5): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (4, 6): tensor(0.0037, grad_fn=<NormBackward0>),\n",
       " (4, 7): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (4, 8): tensor(0.0041, grad_fn=<NormBackward0>),\n",
       " (4, 9): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (4, 10): tensor(0.0056, grad_fn=<NormBackward0>),\n",
       " (4, 11): tensor(0.0029, grad_fn=<NormBackward0>),\n",
       " (4, 12): tensor(0.0033, grad_fn=<NormBackward0>),\n",
       " (4, 13): tensor(0.0033, grad_fn=<NormBackward0>),\n",
       " (4, 14): tensor(0.0024, grad_fn=<NormBackward0>),\n",
       " (4, 15): tensor(0.0020, grad_fn=<NormBackward0>),\n",
       " (4, 16): tensor(0.0029, grad_fn=<NormBackward0>),\n",
       " (4, 17): tensor(0.0045, grad_fn=<NormBackward0>),\n",
       " (4, 18): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (4, 19): tensor(0.0041, grad_fn=<NormBackward0>),\n",
       " (4, 20): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (4, 21): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (4, 22): tensor(0.0039, grad_fn=<NormBackward0>),\n",
       " (4, 23): tensor(0.0044, grad_fn=<NormBackward0>),\n",
       " (4, 24): tensor(0.0027, grad_fn=<NormBackward0>),\n",
       " (4, 25): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (4, 26): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (4, 27): tensor(0.0044, grad_fn=<NormBackward0>),\n",
       " (4, 28): tensor(0.0037, grad_fn=<NormBackward0>),\n",
       " (4, 29): tensor(0.0022, grad_fn=<NormBackward0>),\n",
       " (4, 30): tensor(0.0032, grad_fn=<NormBackward0>),\n",
       " (4, 31): tensor(0.0056, grad_fn=<NormBackward0>),\n",
       " (4, 32): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (4, 33): tensor(0.0027, grad_fn=<NormBackward0>),\n",
       " (4, 34): tensor(0.0015, grad_fn=<NormBackward0>),\n",
       " (4, 35): tensor(0.0059, grad_fn=<NormBackward0>),\n",
       " (4, 36): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (4, 37): tensor(0.0048, grad_fn=<NormBackward0>),\n",
       " (4, 38): tensor(0.0034, grad_fn=<NormBackward0>),\n",
       " (4, 39): tensor(0.0050, grad_fn=<NormBackward0>),\n",
       " (4, 40): tensor(0.0016, grad_fn=<NormBackward0>),\n",
       " (4, 41): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (4, 42): tensor(0.0039, grad_fn=<NormBackward0>),\n",
       " (4, 43): tensor(0.0037, grad_fn=<NormBackward0>),\n",
       " (4, 44): tensor(0.0026, grad_fn=<NormBackward0>),\n",
       " (4, 45): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (4, 46): tensor(0.0020, grad_fn=<NormBackward0>),\n",
       " (4, 47): tensor(0.0030, grad_fn=<NormBackward0>),\n",
       " (4, 48): tensor(0.0048, grad_fn=<NormBackward0>),\n",
       " (4, 49): tensor(0.0048, grad_fn=<NormBackward0>),\n",
       " (4, 50): tensor(0.0012, grad_fn=<NormBackward0>),\n",
       " (4, 51): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (4, 52): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (4, 53): tensor(0.0052, grad_fn=<NormBackward0>),\n",
       " (4, 54): tensor(0.0036, grad_fn=<NormBackward0>),\n",
       " (4, 55): tensor(0.0047, grad_fn=<NormBackward0>),\n",
       " (4, 56): tensor(0.0032, grad_fn=<NormBackward0>),\n",
       " (4, 57): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (4, 58): tensor(0.0030, grad_fn=<NormBackward0>),\n",
       " (4, 59): tensor(0.0031, grad_fn=<NormBackward0>),\n",
       " (4, 60): tensor(0.0041, grad_fn=<NormBackward0>),\n",
       " (4, 61): tensor(0.0017, grad_fn=<NormBackward0>),\n",
       " (4, 62): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (4, 63): tensor(0.0019, grad_fn=<NormBackward0>),\n",
       " (4, 64): tensor(0.0036, grad_fn=<NormBackward0>),\n",
       " (4, 65): tensor(0.0062, grad_fn=<NormBackward0>),\n",
       " (4, 66): tensor(0.0062, grad_fn=<NormBackward0>),\n",
       " (4, 67): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (4, 68): tensor(0.0024, grad_fn=<NormBackward0>),\n",
       " (4, 69): tensor(0.0020, grad_fn=<NormBackward0>),\n",
       " (4, 70): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (4, 71): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (4, 72): tensor(0.0034, grad_fn=<NormBackward0>),\n",
       " (4, 73): tensor(0.0059, grad_fn=<NormBackward0>),\n",
       " (4, 74): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (4, 75): tensor(0.0033, grad_fn=<NormBackward0>),\n",
       " (4, 76): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (4, 77): tensor(0.0042, grad_fn=<NormBackward0>),\n",
       " (4, 78): tensor(0.0036, grad_fn=<NormBackward0>),\n",
       " (4, 79): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (4, 80): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (4, 81): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (4, 82): tensor(0.0037, grad_fn=<NormBackward0>),\n",
       " (4, 83): tensor(0.0031, grad_fn=<NormBackward0>),\n",
       " (4, 84): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (4, 85): tensor(0.0045, grad_fn=<NormBackward0>),\n",
       " (4, 86): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (4, 87): tensor(0.0074, grad_fn=<NormBackward0>),\n",
       " (4, 88): tensor(0.0052, grad_fn=<NormBackward0>),\n",
       " (4, 89): tensor(0.0045, grad_fn=<NormBackward0>),\n",
       " (4, 90): tensor(0.0050, grad_fn=<NormBackward0>),\n",
       " (4, 91): tensor(0.0018, grad_fn=<NormBackward0>),\n",
       " (4, 92): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (4, 93): tensor(0.0044, grad_fn=<NormBackward0>),\n",
       " (4, 94): tensor(0.0050, grad_fn=<NormBackward0>),\n",
       " (4, 95): tensor(0.0020, grad_fn=<NormBackward0>),\n",
       " (4, 96): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (4, 97): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (4, 98): tensor(0.0045, grad_fn=<NormBackward0>),\n",
       " (4, 99): tensor(0.0024, grad_fn=<NormBackward0>),\n",
       " (5, 6): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 7): tensor(0.0035, grad_fn=<NormBackward0>),\n",
       " (5, 8): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 9): tensor(0.0031, grad_fn=<NormBackward0>),\n",
       " (5, 10): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (5, 11): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 12): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 13): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 14): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 15): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 16): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 17): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (5, 18): tensor(0.0026, grad_fn=<NormBackward0>),\n",
       " (5, 19): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (5, 20): tensor(0.0037, grad_fn=<NormBackward0>),\n",
       " (5, 21): tensor(0.0046, grad_fn=<NormBackward0>),\n",
       " (5, 22): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 23): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 24): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 25): tensor(0.0033, grad_fn=<NormBackward0>),\n",
       " (5, 26): tensor(0.0037, grad_fn=<NormBackward0>),\n",
       " (5, 27): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 28): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 29): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 30): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 31): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (5, 32): tensor(0.0040, grad_fn=<NormBackward0>),\n",
       " (5, 33): tensor(16.6318, grad_fn=<NormBackward0>),\n",
       " (5, 34): tensor(16.6318, grad_fn=<NormBackward0>),\n",
       " (5, 35): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 36): tensor(0.0030, grad_fn=<NormBackward0>),\n",
       " (5, 37): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 38): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 39): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 40): tensor(16.6318, grad_fn=<NormBackward0>),\n",
       " (5, 41): tensor(0.0035, grad_fn=<NormBackward0>),\n",
       " (5, 42): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (5, 43): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 44): tensor(16.6318, grad_fn=<NormBackward0>),\n",
       " (5, 45): tensor(0.0034, grad_fn=<NormBackward0>),\n",
       " (5, 46): tensor(16.6318, grad_fn=<NormBackward0>),\n",
       " (5, 47): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 48): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 49): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (5, 50): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 51): tensor(0.0023, grad_fn=<NormBackward0>),\n",
       " (5, 52): tensor(0.0047, grad_fn=<NormBackward0>),\n",
       " (5, 53): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 54): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (5, 55): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 56): tensor(16.6318, grad_fn=<NormBackward0>),\n",
       " (5, 57): tensor(0.0031, grad_fn=<NormBackward0>),\n",
       " (5, 58): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (5, 59): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 60): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 61): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 62): tensor(0.0044, grad_fn=<NormBackward0>),\n",
       " (5, 63): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 64): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 65): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (5, 66): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (5, 67): tensor(0.0055, grad_fn=<NormBackward0>),\n",
       " (5, 68): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 69): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 70): tensor(0.0037, grad_fn=<NormBackward0>),\n",
       " (5, 71): tensor(0.0033, grad_fn=<NormBackward0>),\n",
       " (5, 72): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 73): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (5, 74): tensor(0.0033, grad_fn=<NormBackward0>),\n",
       " (5, 75): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 76): tensor(0.0068, grad_fn=<NormBackward0>),\n",
       " (5, 77): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 78): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 79): tensor(0.0036, grad_fn=<NormBackward0>),\n",
       " (5, 80): tensor(0.0043, grad_fn=<NormBackward0>),\n",
       " (5, 81): tensor(0.0032, grad_fn=<NormBackward0>),\n",
       " (5, 82): tensor(16.6318, grad_fn=<NormBackward0>),\n",
       " (5, 83): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 84): tensor(0.0024, grad_fn=<NormBackward0>),\n",
       " (5, 85): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 86): tensor(0.0035, grad_fn=<NormBackward0>),\n",
       " (5, 87): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (5, 88): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 89): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 90): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (5, 91): tensor(16.6318, grad_fn=<NormBackward0>),\n",
       " (5, 92): tensor(0.0039, grad_fn=<NormBackward0>),\n",
       " (5, 93): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 94): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 95): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 96): tensor(0.0029, grad_fn=<NormBackward0>),\n",
       " (5, 97): tensor(0.0024, grad_fn=<NormBackward0>),\n",
       " (5, 98): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (5, 99): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (6, 7): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (6, 8): tensor(0.0030, grad_fn=<NormBackward0>),\n",
       " (6, 9): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (6, 10): tensor(0.0050, grad_fn=<NormBackward0>),\n",
       " (6, 11): tensor(0.0025, grad_fn=<NormBackward0>),\n",
       " (6, 12): tensor(0.0029, grad_fn=<NormBackward0>),\n",
       " (6, 13): tensor(0.0053, grad_fn=<NormBackward0>),\n",
       " (6, 14): tensor(0.0025, grad_fn=<NormBackward0>),\n",
       " (6, 15): tensor(0.0026, grad_fn=<NormBackward0>),\n",
       " (6, 16): tensor(0.0049, grad_fn=<NormBackward0>),\n",
       " (6, 17): tensor(0.0044, grad_fn=<NormBackward0>),\n",
       " (6, 18): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (6, 19): tensor(0.0026, grad_fn=<NormBackward0>),\n",
       " (6, 20): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (6, 21): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (6, 22): tensor(0.0064, grad_fn=<NormBackward0>),\n",
       " (6, 23): tensor(0.0034, grad_fn=<NormBackward0>),\n",
       " (6, 24): tensor(0.0040, grad_fn=<NormBackward0>),\n",
       " (6, 25): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (6, 26): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (6, 27): tensor(0.0035, grad_fn=<NormBackward0>),\n",
       " (6, 28): tensor(0.0027, grad_fn=<NormBackward0>),\n",
       " (6, 29): tensor(0.0051, grad_fn=<NormBackward0>),\n",
       " (6, 30): tensor(0.0059, grad_fn=<NormBackward0>),\n",
       " (6, 31): tensor(0.0052, grad_fn=<NormBackward0>),\n",
       " (6, 32): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (6, 33): tensor(0.0031, grad_fn=<NormBackward0>),\n",
       " (6, 34): tensor(0.0036, grad_fn=<NormBackward0>),\n",
       " (6, 35): tensor(0.0067, grad_fn=<NormBackward0>),\n",
       " (6, 36): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (6, 37): tensor(0.0076, grad_fn=<NormBackward0>),\n",
       " (6, 38): tensor(0.0022, grad_fn=<NormBackward0>),\n",
       " (6, 39): tensor(0.0045, grad_fn=<NormBackward0>),\n",
       " (6, 40): tensor(0.0043, grad_fn=<NormBackward0>),\n",
       " (6, 41): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (6, 42): tensor(0.0034, grad_fn=<NormBackward0>),\n",
       " (6, 43): tensor(0.0027, grad_fn=<NormBackward0>),\n",
       " (6, 44): tensor(0.0054, grad_fn=<NormBackward0>),\n",
       " (6, 45): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (6, 46): tensor(0.0036, grad_fn=<NormBackward0>),\n",
       " (6, 47): tensor(0.0049, grad_fn=<NormBackward0>),\n",
       " (6, 48): tensor(0.0066, grad_fn=<NormBackward0>),\n",
       " (6, 49): tensor(0.0023, grad_fn=<NormBackward0>),\n",
       " (6, 50): tensor(0.0044, grad_fn=<NormBackward0>),\n",
       " (6, 51): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (6, 52): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (6, 53): tensor(0.0035, grad_fn=<NormBackward0>),\n",
       " (6, 54): tensor(0.0047, grad_fn=<NormBackward0>),\n",
       " (6, 55): tensor(0.0037, grad_fn=<NormBackward0>),\n",
       " (6, 56): tensor(0.0035, grad_fn=<NormBackward0>),\n",
       " (6, 57): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (6, 58): tensor(0.0019, grad_fn=<NormBackward0>),\n",
       " (6, 59): tensor(0.0023, grad_fn=<NormBackward0>),\n",
       " (6, 60): tensor(0.0029, grad_fn=<NormBackward0>),\n",
       " (6, 61): tensor(0.0032, grad_fn=<NormBackward0>),\n",
       " (6, 62): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (6, 63): tensor(0.0045, grad_fn=<NormBackward0>),\n",
       " (6, 64): tensor(0.0055, grad_fn=<NormBackward0>),\n",
       " (6, 65): tensor(0.0054, grad_fn=<NormBackward0>),\n",
       " (6, 66): tensor(0.0048, grad_fn=<NormBackward0>),\n",
       " (6, 67): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (6, 68): tensor(0.0034, grad_fn=<NormBackward0>),\n",
       " (6, 69): tensor(0.0034, grad_fn=<NormBackward0>),\n",
       " (6, 70): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (6, 71): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (6, 72): tensor(0.0027, grad_fn=<NormBackward0>),\n",
       " (6, 73): tensor(0.0049, grad_fn=<NormBackward0>),\n",
       " (6, 74): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (6, 75): tensor(0.0060, grad_fn=<NormBackward0>),\n",
       " (6, 76): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (6, 77): tensor(0.0028, grad_fn=<NormBackward0>),\n",
       " (6, 78): tensor(0.0067, grad_fn=<NormBackward0>),\n",
       " (6, 79): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (6, 80): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (6, 81): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (6, 82): tensor(0.0062, grad_fn=<NormBackward0>),\n",
       " (6, 83): tensor(0.0033, grad_fn=<NormBackward0>),\n",
       " (6, 84): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (6, 85): tensor(0.0044, grad_fn=<NormBackward0>),\n",
       " (6, 86): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (6, 87): tensor(0.0051, grad_fn=<NormBackward0>),\n",
       " (6, 88): tensor(0.0042, grad_fn=<NormBackward0>),\n",
       " (6, 89): tensor(0.0026, grad_fn=<NormBackward0>),\n",
       " (6, 90): tensor(0.0032, grad_fn=<NormBackward0>),\n",
       " (6, 91): tensor(0.0033, grad_fn=<NormBackward0>),\n",
       " (6, 92): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (6, 93): tensor(0.0044, grad_fn=<NormBackward0>),\n",
       " (6, 94): tensor(0.0035, grad_fn=<NormBackward0>),\n",
       " (6, 95): tensor(0.0029, grad_fn=<NormBackward0>),\n",
       " (6, 96): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (6, 97): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (6, 98): tensor(0.0073, grad_fn=<NormBackward0>),\n",
       " (6, 99): tensor(0.0027, grad_fn=<NormBackward0>),\n",
       " (7, 8): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 9): tensor(0.0021, grad_fn=<NormBackward0>),\n",
       " (7, 10): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 11): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 12): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 13): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 14): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 15): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 16): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 17): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 18): tensor(0.0025, grad_fn=<NormBackward0>),\n",
       " (7, 19): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 20): tensor(0.0051, grad_fn=<NormBackward0>),\n",
       " (7, 21): tensor(0.0047, grad_fn=<NormBackward0>),\n",
       " (7, 22): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 23): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 24): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 25): tensor(0.0048, grad_fn=<NormBackward0>),\n",
       " (7, 26): tensor(0.0023, grad_fn=<NormBackward0>),\n",
       " (7, 27): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 28): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 29): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 30): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 31): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 32): tensor(0.0029, grad_fn=<NormBackward0>),\n",
       " (7, 33): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 34): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 35): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 36): tensor(0.0049, grad_fn=<NormBackward0>),\n",
       " (7, 37): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 38): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 39): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 40): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 41): tensor(0.0035, grad_fn=<NormBackward0>),\n",
       " (7, 42): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 43): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 44): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 45): tensor(0.0045, grad_fn=<NormBackward0>),\n",
       " (7, 46): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 47): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 48): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 49): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 50): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 51): tensor(0.0033, grad_fn=<NormBackward0>),\n",
       " (7, 52): tensor(0.0050, grad_fn=<NormBackward0>),\n",
       " (7, 53): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 54): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 55): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 56): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 57): tensor(0.0024, grad_fn=<NormBackward0>),\n",
       " (7, 58): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 59): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 60): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 61): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 62): tensor(0.0034, grad_fn=<NormBackward0>),\n",
       " (7, 63): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 64): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 65): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (7, 66): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 67): tensor(0.0073, grad_fn=<NormBackward0>),\n",
       " (7, 68): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 69): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 70): tensor(0.0046, grad_fn=<NormBackward0>),\n",
       " (7, 71): tensor(0.0023, grad_fn=<NormBackward0>),\n",
       " (7, 72): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 73): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 74): tensor(0.0047, grad_fn=<NormBackward0>),\n",
       " (7, 75): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 76): tensor(0.0078, grad_fn=<NormBackward0>),\n",
       " (7, 77): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 78): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 79): tensor(0.0045, grad_fn=<NormBackward0>),\n",
       " (7, 80): tensor(0.0065, grad_fn=<NormBackward0>),\n",
       " (7, 81): tensor(0.0032, grad_fn=<NormBackward0>),\n",
       " (7, 82): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 83): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 84): tensor(0.0031, grad_fn=<NormBackward0>),\n",
       " (7, 85): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 86): tensor(0.0032, grad_fn=<NormBackward0>),\n",
       " (7, 87): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (7, 88): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 89): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 90): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 91): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 92): tensor(0.0047, grad_fn=<NormBackward0>),\n",
       " (7, 93): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 94): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (7, 95): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 96): tensor(0.0023, grad_fn=<NormBackward0>),\n",
       " (7, 97): tensor(0.0027, grad_fn=<NormBackward0>),\n",
       " (7, 98): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (7, 99): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (8, 9): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (8, 10): tensor(0.0047, grad_fn=<NormBackward0>),\n",
       " (8, 11): tensor(0.0021, grad_fn=<NormBackward0>),\n",
       " (8, 12): tensor(0.0043, grad_fn=<NormBackward0>),\n",
       " (8, 13): tensor(0.0066, grad_fn=<NormBackward0>),\n",
       " (8, 14): tensor(0.0023, grad_fn=<NormBackward0>),\n",
       " (8, 15): tensor(0.0025, grad_fn=<NormBackward0>),\n",
       " (8, 16): tensor(0.0042, grad_fn=<NormBackward0>),\n",
       " (8, 17): tensor(0.0031, grad_fn=<NormBackward0>),\n",
       " (8, 18): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (8, 19): tensor(0.0019, grad_fn=<NormBackward0>),\n",
       " (8, 20): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (8, 21): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (8, 22): tensor(0.0066, grad_fn=<NormBackward0>),\n",
       " (8, 23): tensor(0.0023, grad_fn=<NormBackward0>),\n",
       " (8, 24): tensor(0.0034, grad_fn=<NormBackward0>),\n",
       " (8, 25): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (8, 26): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (8, 27): tensor(0.0010, grad_fn=<NormBackward0>),\n",
       " (8, 28): tensor(0.0015, grad_fn=<NormBackward0>),\n",
       " (8, 29): tensor(0.0056, grad_fn=<NormBackward0>),\n",
       " (8, 30): tensor(0.0051, grad_fn=<NormBackward0>),\n",
       " (8, 31): tensor(0.0037, grad_fn=<NormBackward0>),\n",
       " (8, 32): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (8, 33): tensor(0.0039, grad_fn=<NormBackward0>),\n",
       " (8, 34): tensor(0.0034, grad_fn=<NormBackward0>),\n",
       " (8, 35): tensor(0.0055, grad_fn=<NormBackward0>),\n",
       " (8, 36): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (8, 37): tensor(0.0078, grad_fn=<NormBackward0>),\n",
       " (8, 38): tensor(0.0037, grad_fn=<NormBackward0>),\n",
       " (8, 39): tensor(0.0029, grad_fn=<NormBackward0>),\n",
       " (8, 40): tensor(0.0045, grad_fn=<NormBackward0>),\n",
       " (8, 41): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (8, 42): tensor(0.0018, grad_fn=<NormBackward0>),\n",
       " (8, 43): tensor(0.0033, grad_fn=<NormBackward0>),\n",
       " (8, 44): tensor(0.0064, grad_fn=<NormBackward0>),\n",
       " (8, 45): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (8, 46): tensor(0.0044, grad_fn=<NormBackward0>),\n",
       " (8, 47): tensor(0.0064, grad_fn=<NormBackward0>),\n",
       " (8, 48): tensor(0.0069, grad_fn=<NormBackward0>),\n",
       " (8, 49): tensor(0.0023, grad_fn=<NormBackward0>),\n",
       " (8, 50): tensor(0.0048, grad_fn=<NormBackward0>),\n",
       " (8, 51): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (8, 52): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (8, 53): tensor(0.0021, grad_fn=<NormBackward0>),\n",
       " (8, 54): tensor(0.0040, grad_fn=<NormBackward0>),\n",
       " (8, 55): tensor(0.0020, grad_fn=<NormBackward0>),\n",
       " (8, 56): tensor(0.0027, grad_fn=<NormBackward0>),\n",
       " (8, 57): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (8, 58): tensor(0.0027, grad_fn=<NormBackward0>),\n",
       " (8, 59): tensor(0.0031, grad_fn=<NormBackward0>),\n",
       " (8, 60): tensor(0.0026, grad_fn=<NormBackward0>),\n",
       " (8, 61): tensor(0.0034, grad_fn=<NormBackward0>),\n",
       " (8, 62): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (8, 63): tensor(0.0055, grad_fn=<NormBackward0>),\n",
       " (8, 64): tensor(0.0048, grad_fn=<NormBackward0>),\n",
       " (8, 65): tensor(0.0036, grad_fn=<NormBackward0>),\n",
       " (8, 66): tensor(0.0033, grad_fn=<NormBackward0>),\n",
       " (8, 67): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (8, 68): tensor(0.0022, grad_fn=<NormBackward0>),\n",
       " (8, 69): tensor(0.0035, grad_fn=<NormBackward0>),\n",
       " (8, 70): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (8, 71): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (8, 72): tensor(0.0017, grad_fn=<NormBackward0>),\n",
       " (8, 73): tensor(0.0034, grad_fn=<NormBackward0>),\n",
       " (8, 74): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (8, 75): tensor(0.0065, grad_fn=<NormBackward0>),\n",
       " (8, 76): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (8, 77): tensor(0.0018, grad_fn=<NormBackward0>),\n",
       " (8, 78): tensor(0.0074, grad_fn=<NormBackward0>),\n",
       " (8, 79): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (8, 80): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (8, 81): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (8, 82): tensor(0.0073, grad_fn=<NormBackward0>),\n",
       " (8, 83): tensor(0.0029, grad_fn=<NormBackward0>),\n",
       " (8, 84): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (8, 85): tensor(0.0031, grad_fn=<NormBackward0>),\n",
       " (8, 86): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (8, 87): tensor(0.0036, grad_fn=<NormBackward0>),\n",
       " (8, 88): tensor(0.0018, grad_fn=<NormBackward0>),\n",
       " (8, 89): tensor(0.0030, grad_fn=<NormBackward0>),\n",
       " (8, 90): tensor(0.0020, grad_fn=<NormBackward0>),\n",
       " (8, 91): tensor(0.0029, grad_fn=<NormBackward0>),\n",
       " (8, 92): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (8, 93): tensor(0.0029, grad_fn=<NormBackward0>),\n",
       " (8, 94): tensor(0.0020, grad_fn=<NormBackward0>),\n",
       " (8, 95): tensor(0.0032, grad_fn=<NormBackward0>),\n",
       " (8, 96): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (8, 97): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (8, 98): tensor(0.0081, grad_fn=<NormBackward0>),\n",
       " (8, 99): tensor(0.0024, grad_fn=<NormBackward0>),\n",
       " (9, 10): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 11): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 12): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 13): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 14): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 15): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 16): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 17): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 18): tensor(0.0024, grad_fn=<NormBackward0>),\n",
       " (9, 19): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 20): tensor(0.0048, grad_fn=<NormBackward0>),\n",
       " (9, 21): tensor(0.0037, grad_fn=<NormBackward0>),\n",
       " (9, 22): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 23): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 24): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 25): tensor(0.0036, grad_fn=<NormBackward0>),\n",
       " (9, 26): tensor(0.0018, grad_fn=<NormBackward0>),\n",
       " (9, 27): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 28): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 29): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 30): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 31): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 32): tensor(0.0022, grad_fn=<NormBackward0>),\n",
       " (9, 33): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 34): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 35): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 36): tensor(0.0042, grad_fn=<NormBackward0>),\n",
       " (9, 37): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 38): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 39): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 40): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 41): tensor(0.0023, grad_fn=<NormBackward0>),\n",
       " (9, 42): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 43): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 44): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 45): tensor(0.0042, grad_fn=<NormBackward0>),\n",
       " (9, 46): tensor(16.6318, grad_fn=<NormBackward0>),\n",
       " (9, 47): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 48): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 49): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 50): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 51): tensor(0.0025, grad_fn=<NormBackward0>),\n",
       " (9, 52): tensor(0.0042, grad_fn=<NormBackward0>),\n",
       " (9, 53): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 54): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 55): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 56): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 57): tensor(0.0015, grad_fn=<NormBackward0>),\n",
       " (9, 58): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 59): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 60): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 61): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 62): tensor(0.0028, grad_fn=<NormBackward0>),\n",
       " (9, 63): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 64): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 65): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (9, 66): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 67): tensor(0.0067, grad_fn=<NormBackward0>),\n",
       " (9, 68): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 69): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 70): tensor(0.0045, grad_fn=<NormBackward0>),\n",
       " (9, 71): tensor(0.0028, grad_fn=<NormBackward0>),\n",
       " (9, 72): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 73): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 74): tensor(0.0037, grad_fn=<NormBackward0>),\n",
       " (9, 75): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 76): tensor(0.0079, grad_fn=<NormBackward0>),\n",
       " (9, 77): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 78): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 79): tensor(0.0035, grad_fn=<NormBackward0>),\n",
       " (9, 80): tensor(0.0058, grad_fn=<NormBackward0>),\n",
       " (9, 81): tensor(0.0023, grad_fn=<NormBackward0>),\n",
       " (9, 82): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 83): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 84): tensor(0.0023, grad_fn=<NormBackward0>),\n",
       " (9, 85): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 86): tensor(0.0024, grad_fn=<NormBackward0>),\n",
       " (9, 87): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (9, 88): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 89): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 90): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 91): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 92): tensor(0.0035, grad_fn=<NormBackward0>),\n",
       " (9, 93): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 94): tensor(16.6317, grad_fn=<NormBackward0>),\n",
       " (9, 95): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 96): tensor(0.0013, grad_fn=<NormBackward0>),\n",
       " (9, 97): tensor(0.0025, grad_fn=<NormBackward0>),\n",
       " (9, 98): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (9, 99): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (10, 11): tensor(0.0041, grad_fn=<NormBackward0>),\n",
       " (10, 12): tensor(0.0045, grad_fn=<NormBackward0>),\n",
       " (10, 13): tensor(0.0067, grad_fn=<NormBackward0>),\n",
       " (10, 14): tensor(0.0054, grad_fn=<NormBackward0>),\n",
       " (10, 15): tensor(0.0048, grad_fn=<NormBackward0>),\n",
       " (10, 16): tensor(0.0048, grad_fn=<NormBackward0>),\n",
       " (10, 17): tensor(0.0029, grad_fn=<NormBackward0>),\n",
       " (10, 18): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (10, 19): tensor(0.0055, grad_fn=<NormBackward0>),\n",
       " (10, 20): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (10, 21): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (10, 22): tensor(0.0054, grad_fn=<NormBackward0>),\n",
       " (10, 23): tensor(0.0062, grad_fn=<NormBackward0>),\n",
       " (10, 24): tensor(0.0069, grad_fn=<NormBackward0>),\n",
       " (10, 25): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (10, 26): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (10, 27): tensor(0.0053, grad_fn=<NormBackward0>),\n",
       " (10, 28): tensor(0.0047, grad_fn=<NormBackward0>),\n",
       " (10, 29): tensor(0.0071, grad_fn=<NormBackward0>),\n",
       " (10, 30): tensor(0.0062, grad_fn=<NormBackward0>),\n",
       " (10, 31): tensor(0.0078, grad_fn=<NormBackward0>),\n",
       " (10, 32): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (10, 33): tensor(0.0048, grad_fn=<NormBackward0>),\n",
       " (10, 34): tensor(0.0062, grad_fn=<NormBackward0>),\n",
       " (10, 35): tensor(0.0095, grad_fn=<NormBackward0>),\n",
       " (10, 36): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (10, 37): tensor(0.0069, grad_fn=<NormBackward0>),\n",
       " (10, 38): tensor(0.0043, grad_fn=<NormBackward0>),\n",
       " (10, 39): tensor(0.0032, grad_fn=<NormBackward0>),\n",
       " (10, 40): tensor(0.0063, grad_fn=<NormBackward0>),\n",
       " (10, 41): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (10, 42): tensor(0.0054, grad_fn=<NormBackward0>),\n",
       " (10, 43): tensor(0.0067, grad_fn=<NormBackward0>),\n",
       " (10, 44): tensor(0.0069, grad_fn=<NormBackward0>),\n",
       " (10, 45): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (10, 46): tensor(0.0061, grad_fn=<NormBackward0>),\n",
       " (10, 47): tensor(0.0072, grad_fn=<NormBackward0>),\n",
       " (10, 48): tensor(0.0055, grad_fn=<NormBackward0>),\n",
       " (10, 49): tensor(0.0043, grad_fn=<NormBackward0>),\n",
       " (10, 50): tensor(0.0062, grad_fn=<NormBackward0>),\n",
       " (10, 51): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (10, 52): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (10, 53): tensor(0.0053, grad_fn=<NormBackward0>),\n",
       " (10, 54): tensor(0.0037, grad_fn=<NormBackward0>),\n",
       " (10, 55): tensor(0.0063, grad_fn=<NormBackward0>),\n",
       " (10, 56): tensor(0.0064, grad_fn=<NormBackward0>),\n",
       " (10, 57): tensor(16.6315, grad_fn=<NormBackward0>),\n",
       " (10, 58): tensor(0.0043, grad_fn=<NormBackward0>),\n",
       " (10, 59): tensor(0.0058, grad_fn=<NormBackward0>),\n",
       " (10, 60): tensor(0.0050, grad_fn=<NormBackward0>),\n",
       " (10, 61): tensor(0.0059, grad_fn=<NormBackward0>),\n",
       " (10, 62): tensor(16.6316, grad_fn=<NormBackward0>),\n",
       " (10, 63): tensor(0.0060, grad_fn=<NormBackward0>),\n",
       " (10, 64): tensor(0.0051, grad_fn=<NormBackward0>),\n",
       " (10, 65): tensor(0.0037, grad_fn=<NormBackward0>),\n",
       " ...}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_vecs = np.array([get_params_vec(m).detach().numpy() for m in models.values()])\n",
    "\n",
    "X_embedded = TSNE(n_components=2).fit_transform(models_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_embedded[:, 0], X_embedded[:, 1])\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = models[str(0)]\n",
    "grad_steps= []\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)  # fix the batch size\n",
    "\n",
    "for data in train_loader:\n",
    "    inputs, labels = data\n",
    "    # Compute gradients for input.\n",
    "    inputs.requires_grad = True\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = m(inputs)\n",
    "    loss = criterion(outputs.float(), labels)\n",
    "    loss.backward(retain_graph=True)\n",
    "    \n",
    "    grad_steps.append(get_grad_params_vec(m).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vh = np.linalg.svd(np.array(grad_steps), full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.4566307e+01, 4.2778692e+00, 3.0374603e+00, 9.4402808e-01,\n",
       "       3.5643500e-01, 2.0473388e-01, 1.5873392e-01, 1.1142698e-01,\n",
       "       9.3603715e-02, 7.8027248e-02, 6.5044165e-02, 6.0796760e-02,\n",
       "       4.8285898e-02, 3.8336921e-02, 3.6647409e-02, 3.3087600e-02,\n",
       "       2.9207643e-02, 2.3398781e-02, 2.1899553e-02, 2.1008376e-02,\n",
       "       1.7819939e-02, 1.6101249e-02, 1.4436195e-02, 1.2744365e-02,\n",
       "       1.1394381e-02, 1.0279431e-02, 9.7109778e-03, 9.3411161e-03,\n",
       "       8.4276162e-03, 8.1074052e-03, 7.4696536e-03, 6.8786601e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = get_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
